{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/Users/aadidass-vattam/Documents/CorselloLab/Cluster/clusters200_with_viability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df[cluster_df['Less than 30% Viable'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pert_name</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>MOA</th>\n",
       "      <th>Less than 20% Viable</th>\n",
       "      <th>Less than 30% Viable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2428</td>\n",
       "      <td>funapide</td>\n",
       "      <td>0</td>\n",
       "      <td>SODIUM CHANNEL BLOCKER</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3237</td>\n",
       "      <td>lifitegrast</td>\n",
       "      <td>0</td>\n",
       "      <td>LYMPHOCYTE FUNCTION-ASSOCIATED ANTIGEN NEGATIV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2320</td>\n",
       "      <td>fk-409</td>\n",
       "      <td>0</td>\n",
       "      <td>GUANYLYL CYCLASE ACTIVATOR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552</td>\n",
       "      <td>argireline</td>\n",
       "      <td>0</td>\n",
       "      <td>NEUROTRANSMITTER RELEASE INHIBITOR</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5918</td>\n",
       "      <td>vatinoxan</td>\n",
       "      <td>0</td>\n",
       "      <td>ADRENERGIC RECEPTOR ANTAGONIST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1204</td>\n",
       "      <td>cbl0137</td>\n",
       "      <td>199</td>\n",
       "      <td>HISTONE CHAPERONE INHIBITOR</td>\n",
       "      <td>888.0</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1567</td>\n",
       "      <td>cpi-169</td>\n",
       "      <td>199</td>\n",
       "      <td>HISTONE LYSINE METHYLTRANSFERASE INHIBITOR</td>\n",
       "      <td>19.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1528</td>\n",
       "      <td>convallatoxin</td>\n",
       "      <td>199</td>\n",
       "      <td>CARDIAC GLYCOSIDE</td>\n",
       "      <td>888.0</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>5517</td>\n",
       "      <td>temefos</td>\n",
       "      <td>199</td>\n",
       "      <td>CHOLINESTERASE INHIBITOR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5390</td>\n",
       "      <td>tak-243</td>\n",
       "      <td>199</td>\n",
       "      <td>UBIQUITIN ACTIVATING ENZYME INHIBITOR</td>\n",
       "      <td>882.0</td>\n",
       "      <td>886.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1142 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      pert_name  Cluster  \\\n",
       "0           2428       funapide        0   \n",
       "1           3237    lifitegrast        0   \n",
       "2           2320         fk-409        0   \n",
       "3            552     argireline        0   \n",
       "4           5918      vatinoxan        0   \n",
       "...          ...            ...      ...   \n",
       "1137        1204        cbl0137      199   \n",
       "1138        1567        cpi-169      199   \n",
       "1139        1528  convallatoxin      199   \n",
       "1140        5517        temefos      199   \n",
       "1141        5390        tak-243      199   \n",
       "\n",
       "                                                    MOA  Less than 20% Viable  \\\n",
       "0                                SODIUM CHANNEL BLOCKER                   1.0   \n",
       "1     LYMPHOCYTE FUNCTION-ASSOCIATED ANTIGEN NEGATIV...                   1.0   \n",
       "2                            GUANYLYL CYCLASE ACTIVATOR                   3.0   \n",
       "3                    NEUROTRANSMITTER RELEASE INHIBITOR                   2.0   \n",
       "4                        ADRENERGIC RECEPTOR ANTAGONIST                   0.0   \n",
       "...                                                 ...                   ...   \n",
       "1137                        HISTONE CHAPERONE INHIBITOR                 888.0   \n",
       "1138         HISTONE LYSINE METHYLTRANSFERASE INHIBITOR                  19.0   \n",
       "1139                                  CARDIAC GLYCOSIDE                 888.0   \n",
       "1140                           CHOLINESTERASE INHIBITOR                   4.0   \n",
       "1141              UBIQUITIN ACTIVATING ENZYME INHIBITOR                 882.0   \n",
       "\n",
       "      Less than 30% Viable  \n",
       "0                      5.0  \n",
       "1                      6.0  \n",
       "2                      5.0  \n",
       "3                      5.0  \n",
       "4                      2.0  \n",
       "...                    ...  \n",
       "1137                 890.0  \n",
       "1138                  50.0  \n",
       "1139                 890.0  \n",
       "1140                  13.0  \n",
       "1141                 886.0  \n",
       "\n",
       "[1142 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pert_name</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>MOA</th>\n",
       "      <th>Less than 20% Viable</th>\n",
       "      <th>Less than 30% Viable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5810</td>\n",
       "      <td>tucidinostat</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>44.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1444</td>\n",
       "      <td>citarinostat</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5050</td>\n",
       "      <td>sbha</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5483</td>\n",
       "      <td>tcs-2210</td>\n",
       "      <td>8</td>\n",
       "      <td>NEURAL STEM CELL INDUCER</td>\n",
       "      <td>12.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5461</td>\n",
       "      <td>tc-h-106</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1610</td>\n",
       "      <td>cxd101</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>440.0</td>\n",
       "      <td>574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2635</td>\n",
       "      <td>gsk3117391</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>644.0</td>\n",
       "      <td>748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3269</td>\n",
       "      <td>lmk-235</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>88.0</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5852</td>\n",
       "      <td>uf-010</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3495</td>\n",
       "      <td>merck60</td>\n",
       "      <td>8</td>\n",
       "      <td>HDAC INHIBITOR</td>\n",
       "      <td>274.0</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     pert_name  Cluster                       MOA  \\\n",
       "37        5810  tucidinostat        8            HDAC INHIBITOR   \n",
       "38        1444  citarinostat        8            HDAC INHIBITOR   \n",
       "39        5050          sbha        8            HDAC INHIBITOR   \n",
       "40        5483      tcs-2210        8  NEURAL STEM CELL INDUCER   \n",
       "41        5461      tc-h-106        8            HDAC INHIBITOR   \n",
       "42        1610        cxd101        8            HDAC INHIBITOR   \n",
       "43        2635    gsk3117391        8            HDAC INHIBITOR   \n",
       "44        3269       lmk-235        8            HDAC INHIBITOR   \n",
       "45        5852        uf-010        8            HDAC INHIBITOR   \n",
       "46        3495       merck60        8            HDAC INHIBITOR   \n",
       "\n",
       "    Less than 20% Viable  Less than 30% Viable  \n",
       "37                  44.0                 124.0  \n",
       "38                   3.0                   8.0  \n",
       "39                   3.0                   9.0  \n",
       "40                  12.0                  39.0  \n",
       "41                  12.0                  38.0  \n",
       "42                 440.0                 574.0  \n",
       "43                 644.0                 748.0  \n",
       "44                  88.0                 225.0  \n",
       "45                   7.0                  15.0  \n",
       "46                 274.0                 394.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df[cluster_df['Cluster'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_ex = cluster_df[cluster_df['Cluster']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_ex = toy_ex.drop_duplicates(subset=['pert_name','MOA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for drug in list(toy_ex['pert_name']):\n",
    "    for file in glob.glob(f'/Users/aadidass-vattam/Documents/CorselloLab/Cluster/responses/{drug}*.csv'):\n",
    "        paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(paths) > 10:\n",
    "    paths = random.sample(paths, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"/Users/aadidass-vattam/Documents/CorselloLab/Cluster/processed_data/x-all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()\n",
    "features = features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ccle_name</th>\n",
       "      <th>GE_TSPAN6</th>\n",
       "      <th>GE_TNMD</th>\n",
       "      <th>GE_DPM1</th>\n",
       "      <th>GE_SCYL3</th>\n",
       "      <th>GE_C1orf112</th>\n",
       "      <th>GE_FGR</th>\n",
       "      <th>GE_CFH</th>\n",
       "      <th>GE_FUCA2</th>\n",
       "      <th>GE_GCLC</th>\n",
       "      <th>...</th>\n",
       "      <th>MET_C56:8 TAG</th>\n",
       "      <th>MET_C56:7 TAG</th>\n",
       "      <th>MET_C56:6 TAG</th>\n",
       "      <th>MET_C56:5 TAG</th>\n",
       "      <th>MET_C56:4 TAG</th>\n",
       "      <th>MET_C56:3 TAG</th>\n",
       "      <th>MET_C56:2 TAG</th>\n",
       "      <th>MET_C58:8 TAG</th>\n",
       "      <th>MET_C58:7 TAG</th>\n",
       "      <th>MET_C58:6 TAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127399_SOFT_TISSUE</td>\n",
       "      <td>3.243364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.863567</td>\n",
       "      <td>2.063503</td>\n",
       "      <td>3.916477</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>5.426600</td>\n",
       "      <td>3.820690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1321N1_CENTRAL_NERVOUS_SYSTEM</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143B_BONE</td>\n",
       "      <td>4.719731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.360540</td>\n",
       "      <td>1.778209</td>\n",
       "      <td>3.786596</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.713696</td>\n",
       "      <td>5.393348</td>\n",
       "      <td>4.638074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170MGBA_CENTRAL_NERVOUS_SYSTEM</td>\n",
       "      <td>4.155425</td>\n",
       "      <td>0.124328</td>\n",
       "      <td>6.681590</td>\n",
       "      <td>2.430285</td>\n",
       "      <td>2.799087</td>\n",
       "      <td>0.815575</td>\n",
       "      <td>3.559492</td>\n",
       "      <td>7.252855</td>\n",
       "      <td>4.300856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184A1_BREAST</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ccle_name  GE_TSPAN6   GE_TNMD   GE_DPM1  GE_SCYL3  \\\n",
       "0              127399_SOFT_TISSUE   3.243364  0.000000  6.863567  2.063503   \n",
       "1   1321N1_CENTRAL_NERVOUS_SYSTEM   0.000000  0.000000  0.000000  0.000000   \n",
       "2                       143B_BONE   4.719731  0.000000  7.360540  1.778209   \n",
       "3  170MGBA_CENTRAL_NERVOUS_SYSTEM   4.155425  0.124328  6.681590  2.430285   \n",
       "4                    184A1_BREAST   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   GE_C1orf112    GE_FGR    GE_CFH  GE_FUCA2   GE_GCLC  ...  MET_C56:8 TAG  \\\n",
       "0     3.916477  0.070389  0.056584  5.426600  3.820690  ...            0.0   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.000000  ...            0.0   \n",
       "2     3.786596  0.014355  0.713696  5.393348  4.638074  ...            0.0   \n",
       "3     2.799087  0.815575  3.559492  7.252855  4.300856  ...            0.0   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  ...            0.0   \n",
       "\n",
       "   MET_C56:7 TAG  MET_C56:6 TAG  MET_C56:5 TAG  MET_C56:4 TAG  MET_C56:3 TAG  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   MET_C56:2 TAG  MET_C58:8 TAG  MET_C58:7 TAG  MET_C58:6 TAG  \n",
       "0            0.0            0.0            0.0            0.0  \n",
       "1            0.0            0.0            0.0            0.0  \n",
       "2            0.0            0.0            0.0            0.0  \n",
       "3            0.0            0.0            0.0            0.0  \n",
       "4            0.0            0.0            0.0            0.0  \n",
       "\n",
       "[5 rows x 110136 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "count = 0\n",
    "for file_path in paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['LFC.cb','ccle_name']]\n",
    "    df = df.merge(features, on=\"ccle_name\")\n",
    "    df = df.drop(columns=['ccle_name'])\n",
    "    df['ID'] = count\n",
    "    data_frames.append(df)\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/refract/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/envs/refract/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "corr = dict(df.corrwith(df['LFC.cb']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = sorted(corr, key=lambda k: abs(corr[k]), reverse=True)[:502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ID' not in top_features:\n",
    "    top_features = top_features[:501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.append('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LFC.cb</th>\n",
       "      <th>GE_S100A6</th>\n",
       "      <th>GE_LIN28B</th>\n",
       "      <th>GE_ITGA3</th>\n",
       "      <th>GE_LGALS3</th>\n",
       "      <th>GE_SPATS2L</th>\n",
       "      <th>GE_LIF</th>\n",
       "      <th>GE_SYTL2</th>\n",
       "      <th>GE_TRIM71</th>\n",
       "      <th>GE_NCEH1</th>\n",
       "      <th>...</th>\n",
       "      <th>GE_FCHO1</th>\n",
       "      <th>GE_ITGB1</th>\n",
       "      <th>GE_PLAAT4</th>\n",
       "      <th>GE_PTPN12</th>\n",
       "      <th>GE_TSPAN7</th>\n",
       "      <th>GE_SBK1</th>\n",
       "      <th>GE_MYBL1</th>\n",
       "      <th>GE_CSAG1</th>\n",
       "      <th>GE_SKOR1</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.275388</td>\n",
       "      <td>11.162580</td>\n",
       "      <td>2.100978</td>\n",
       "      <td>7.009885</td>\n",
       "      <td>8.004838</td>\n",
       "      <td>6.171727</td>\n",
       "      <td>4.375735</td>\n",
       "      <td>4.473787</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>1.757023</td>\n",
       "      <td>...</td>\n",
       "      <td>3.531069</td>\n",
       "      <td>8.331454</td>\n",
       "      <td>1.622930</td>\n",
       "      <td>5.356144</td>\n",
       "      <td>2.750607</td>\n",
       "      <td>2.918386</td>\n",
       "      <td>2.104337</td>\n",
       "      <td>0.214125</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.717885</td>\n",
       "      <td>5.500483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.731183</td>\n",
       "      <td>2.266037</td>\n",
       "      <td>0.454176</td>\n",
       "      <td>0.526069</td>\n",
       "      <td>0.505891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.650765</td>\n",
       "      <td>...</td>\n",
       "      <td>3.422233</td>\n",
       "      <td>3.954196</td>\n",
       "      <td>0.411426</td>\n",
       "      <td>3.778209</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.400538</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.545968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.439110</td>\n",
       "      <td>9.284176</td>\n",
       "      <td>4.488644</td>\n",
       "      <td>0.604071</td>\n",
       "      <td>1.269033</td>\n",
       "      <td>1.195348</td>\n",
       "      <td>4.018812</td>\n",
       "      <td>0.226509</td>\n",
       "      <td>1.521051</td>\n",
       "      <td>2.283922</td>\n",
       "      <td>...</td>\n",
       "      <td>3.452859</td>\n",
       "      <td>6.907492</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>4.218006</td>\n",
       "      <td>2.090853</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>1.411426</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.545968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.954180</td>\n",
       "      <td>7.268846</td>\n",
       "      <td>5.350497</td>\n",
       "      <td>0.695994</td>\n",
       "      <td>1.695994</td>\n",
       "      <td>3.431623</td>\n",
       "      <td>2.944858</td>\n",
       "      <td>0.333424</td>\n",
       "      <td>1.782409</td>\n",
       "      <td>2.533563</td>\n",
       "      <td>...</td>\n",
       "      <td>3.193772</td>\n",
       "      <td>7.403182</td>\n",
       "      <td>1.459432</td>\n",
       "      <td>4.869378</td>\n",
       "      <td>3.116032</td>\n",
       "      <td>0.084064</td>\n",
       "      <td>1.459432</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.352701</td>\n",
       "      <td>8.296503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>3.725741</td>\n",
       "      <td>1.695994</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>2.718088</td>\n",
       "      <td>...</td>\n",
       "      <td>3.917432</td>\n",
       "      <td>6.196332</td>\n",
       "      <td>1.650765</td>\n",
       "      <td>5.150560</td>\n",
       "      <td>0.124328</td>\n",
       "      <td>0.310340</td>\n",
       "      <td>1.879706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LFC.cb  GE_S100A6  GE_LIN28B  GE_ITGA3  GE_LGALS3  GE_SPATS2L    GE_LIF  \\\n",
       "0 -1.275388  11.162580   2.100978  7.009885   8.004838    6.171727  4.375735   \n",
       "1 -0.717885   5.500483   0.000000  1.731183   2.266037    0.454176  0.526069   \n",
       "2 -0.439110   9.284176   4.488644  0.604071   1.269033    1.195348  4.018812   \n",
       "3 -1.954180   7.268846   5.350497  0.695994   1.695994    3.431623  2.944858   \n",
       "4 -2.352701   8.296503   0.000000  0.584963   3.725741    1.695994  0.201634   \n",
       "\n",
       "   GE_SYTL2  GE_TRIM71  GE_NCEH1  ...  GE_FCHO1  GE_ITGB1  GE_PLAAT4  \\\n",
       "0  4.473787   0.056584  1.757023  ...  3.531069  8.331454   1.622930   \n",
       "1  0.505891   0.000000  1.650765  ...  3.422233  3.954196   0.411426   \n",
       "2  0.226509   1.521051  2.283922  ...  3.452859  6.907492   1.176323   \n",
       "3  0.333424   1.782409  2.533563  ...  3.193772  7.403182   1.459432   \n",
       "4  0.485427   0.056584  2.718088  ...  3.917432  6.196332   1.650765   \n",
       "\n",
       "   GE_PTPN12  GE_TSPAN7   GE_SBK1  GE_MYBL1  GE_CSAG1  GE_SKOR1  ID  \n",
       "0   5.356144   2.750607  2.918386  2.104337  0.214125  0.028569   0  \n",
       "1   3.778209   0.042644  0.028569  0.400538  0.056584  0.545968   0  \n",
       "2   4.218006   2.090853  0.056584  1.411426  0.263034  0.545968   0  \n",
       "3   4.869378   3.116032  0.084064  1.459432  0.070389  0.201634   0  \n",
       "4   5.150560   0.124328  0.310340  1.879706  0.000000  0.201634   0  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"toy_data_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1.275388\n",
       "1     -0.717885\n",
       "2     -0.439110\n",
       "3     -1.954180\n",
       "4     -2.352701\n",
       "         ...   \n",
       "896   -0.718682\n",
       "897   -1.078348\n",
       "898   -1.932802\n",
       "899    0.013111\n",
       "900   -5.983090\n",
       "Name: LFC.cb, Length: 8984, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ID'] = data['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_S100A6</th>\n",
       "      <th>GE_LIN28B</th>\n",
       "      <th>GE_ITGA3</th>\n",
       "      <th>GE_LGALS3</th>\n",
       "      <th>GE_SPATS2L</th>\n",
       "      <th>GE_LIF</th>\n",
       "      <th>GE_SYTL2</th>\n",
       "      <th>GE_TRIM71</th>\n",
       "      <th>GE_NCEH1</th>\n",
       "      <th>GE_TM4SF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GE_FCHO1</th>\n",
       "      <th>GE_ITGB1</th>\n",
       "      <th>GE_PLAAT4</th>\n",
       "      <th>GE_PTPN12</th>\n",
       "      <th>GE_TSPAN7</th>\n",
       "      <th>GE_SBK1</th>\n",
       "      <th>GE_MYBL1</th>\n",
       "      <th>GE_CSAG1</th>\n",
       "      <th>GE_SKOR1</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.366488</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.406031</td>\n",
       "      <td>0.702490</td>\n",
       "      <td>0.307160</td>\n",
       "      <td>0.465135</td>\n",
       "      <td>0.701713</td>\n",
       "      <td>-0.271155</td>\n",
       "      <td>-1.148424</td>\n",
       "      <td>1.050072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806268</td>\n",
       "      <td>0.258499</td>\n",
       "      <td>-0.598159</td>\n",
       "      <td>0.209721</td>\n",
       "      <td>0.585603</td>\n",
       "      <td>1.197076</td>\n",
       "      <td>-0.574523</td>\n",
       "      <td>-0.611186</td>\n",
       "      <td>-0.514383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.264284</td>\n",
       "      <td>-0.553618</td>\n",
       "      <td>-1.489424</td>\n",
       "      <td>-1.341092</td>\n",
       "      <td>-2.733046</td>\n",
       "      <td>-1.305679</td>\n",
       "      <td>-1.192501</td>\n",
       "      <td>-0.338397</td>\n",
       "      <td>-1.212502</td>\n",
       "      <td>-1.356434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739144</td>\n",
       "      <td>-2.353373</td>\n",
       "      <td>-1.112870</td>\n",
       "      <td>-1.209354</td>\n",
       "      <td>-0.752649</td>\n",
       "      <td>-0.862205</td>\n",
       "      <td>-1.807553</td>\n",
       "      <td>-0.676921</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.174522</td>\n",
       "      <td>2.721393</td>\n",
       "      <td>-1.894143</td>\n",
       "      <td>-1.696125</td>\n",
       "      <td>-2.338941</td>\n",
       "      <td>0.300954</td>\n",
       "      <td>-1.325874</td>\n",
       "      <td>1.469179</td>\n",
       "      <td>-0.830686</td>\n",
       "      <td>-0.880767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758032</td>\n",
       "      <td>-0.591168</td>\n",
       "      <td>-0.787902</td>\n",
       "      <td>-0.813834</td>\n",
       "      <td>0.259559</td>\n",
       "      <td>-0.842242</td>\n",
       "      <td>-1.075979</td>\n",
       "      <td>-0.590778</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.754968</td>\n",
       "      <td>3.350220</td>\n",
       "      <td>-1.861136</td>\n",
       "      <td>-1.544084</td>\n",
       "      <td>-1.149841</td>\n",
       "      <td>-0.193056</td>\n",
       "      <td>-1.274835</td>\n",
       "      <td>1.779769</td>\n",
       "      <td>-0.680143</td>\n",
       "      <td>-0.539810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598243</td>\n",
       "      <td>-0.295393</td>\n",
       "      <td>-0.667622</td>\n",
       "      <td>-0.228039</td>\n",
       "      <td>0.766194</td>\n",
       "      <td>-0.822660</td>\n",
       "      <td>-1.041237</td>\n",
       "      <td>-0.671161</td>\n",
       "      <td>-0.050954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.458987</td>\n",
       "      <td>-0.553618</td>\n",
       "      <td>-1.901005</td>\n",
       "      <td>-0.821293</td>\n",
       "      <td>-2.072731</td>\n",
       "      <td>-1.454916</td>\n",
       "      <td>-1.202271</td>\n",
       "      <td>-0.271155</td>\n",
       "      <td>-0.568868</td>\n",
       "      <td>-0.960492</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044552</td>\n",
       "      <td>-1.015511</td>\n",
       "      <td>-0.586333</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>-0.712282</td>\n",
       "      <td>-0.661416</td>\n",
       "      <td>-0.737087</td>\n",
       "      <td>-0.700531</td>\n",
       "      <td>-0.050954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GE_S100A6  GE_LIN28B  GE_ITGA3  GE_LGALS3  GE_SPATS2L    GE_LIF  GE_SYTL2  \\\n",
       "0   0.366488   0.979300  0.406031   0.702490    0.307160  0.465135  0.701713   \n",
       "1  -1.264284  -0.553618 -1.489424  -1.341092   -2.733046 -1.305679 -1.192501   \n",
       "2  -0.174522   2.721393 -1.894143  -1.696125   -2.338941  0.300954 -1.325874   \n",
       "3  -0.754968   3.350220 -1.861136  -1.544084   -1.149841 -0.193056 -1.274835   \n",
       "4  -0.458987  -0.553618 -1.901005  -0.821293   -2.072731 -1.454916 -1.202271   \n",
       "\n",
       "   GE_TRIM71  GE_NCEH1  GE_TM4SF1  ...  GE_FCHO1  GE_ITGB1  GE_PLAAT4  \\\n",
       "0  -0.271155 -1.148424   1.050072  ...  0.806268  0.258499  -0.598159   \n",
       "1  -0.338397 -1.212502  -1.356434  ...  0.739144 -2.353373  -1.112870   \n",
       "2   1.469179 -0.830686  -0.880767  ...  0.758032 -0.591168  -0.787902   \n",
       "3   1.779769 -0.680143  -0.539810  ...  0.598243 -0.295393  -0.667622   \n",
       "4  -0.271155 -0.568868  -0.960492  ...  1.044552 -1.015511  -0.586333   \n",
       "\n",
       "   GE_PTPN12  GE_TSPAN7   GE_SBK1  GE_MYBL1  GE_CSAG1  GE_SKOR1  ID  \n",
       "0   0.209721   0.585603  1.197076 -0.574523 -0.611186 -0.514383   0  \n",
       "1  -1.209354  -0.752649 -0.862205 -1.807553 -0.676921  0.871099   0  \n",
       "2  -0.813834   0.259559 -0.842242 -1.075979 -0.590778  0.871099   0  \n",
       "3  -0.228039   0.766194 -0.822660 -1.041237 -0.671161 -0.050954   0  \n",
       "4   0.024834  -0.712282 -0.661416 -0.737087 -0.700531 -0.050954   0  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(features, noise_level=0.01):\n",
    "    noise = torch.randn_like(features) * noise_level\n",
    "    return features + noise\n",
    "\n",
    "def scale_features(features, scale_factor=0.1):\n",
    "    scale = 1 + (torch.randn_like(features) * scale_factor)\n",
    "    return features * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)  # Adding extra dimension for targets\n",
    "        \n",
    "        self.features_noise = add_noise(self.features)\n",
    "        self.features_scaled = scale_features(self.features)\n",
    "        self.features_both = scale_features(add_noise(self.features))\n",
    "        \n",
    "        self.all_features = torch.cat([self.features, self.features_noise, self.features_scaled, self.features_both])\n",
    "        self.all_targets = self.targets.repeat(4, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_features[idx], self.all_targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 501\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/refract/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2029.4399\n",
      "Epoch [1/100], Validation Loss: 2.5283\n",
      "Epoch [2/100], Train Loss: 1777.2849\n",
      "Epoch [2/100], Validation Loss: 2.4847\n",
      "Epoch [3/100], Train Loss: 1616.5028\n",
      "Epoch [3/100], Validation Loss: 2.3039\n",
      "Epoch [4/100], Train Loss: 1473.9116\n",
      "Epoch [4/100], Validation Loss: 2.2138\n",
      "Epoch [5/100], Train Loss: 1415.7000\n",
      "Epoch [5/100], Validation Loss: 2.1481\n",
      "Epoch [6/100], Train Loss: 1337.0980\n",
      "Epoch [6/100], Validation Loss: 2.2202\n",
      "Epoch [7/100], Train Loss: 1277.7161\n",
      "Epoch [7/100], Validation Loss: 2.1861\n",
      "Epoch [8/100], Train Loss: 1226.5889\n",
      "Epoch [8/100], Validation Loss: 2.1516\n",
      "Epoch [9/100], Train Loss: 1201.5018\n",
      "Epoch [9/100], Validation Loss: 2.1458\n",
      "Epoch [10/100], Train Loss: 1169.1893\n",
      "Epoch [10/100], Validation Loss: 2.1408\n",
      "Epoch [11/100], Train Loss: 1164.5233\n",
      "Epoch [11/100], Validation Loss: 2.0710\n",
      "Epoch [12/100], Train Loss: 1134.4556\n",
      "Epoch [12/100], Validation Loss: 2.2485\n",
      "Epoch [13/100], Train Loss: 1121.2345\n",
      "Epoch [13/100], Validation Loss: 2.0792\n",
      "Epoch [14/100], Train Loss: 1110.5525\n",
      "Epoch [14/100], Validation Loss: 2.2220\n",
      "Epoch [15/100], Train Loss: 1096.2577\n",
      "Epoch [15/100], Validation Loss: 2.3180\n",
      "Epoch [16/100], Train Loss: 1065.0409\n",
      "Epoch [16/100], Validation Loss: 2.1985\n",
      "Epoch [17/100], Train Loss: 1073.5576\n",
      "Epoch [17/100], Validation Loss: 2.1414\n",
      "Epoch [18/100], Train Loss: 1079.0001\n",
      "Epoch [18/100], Validation Loss: 2.2012\n",
      "Epoch [19/100], Train Loss: 1067.4579\n",
      "Epoch [19/100], Validation Loss: 2.1929\n",
      "Epoch [20/100], Train Loss: 1065.3843\n",
      "Epoch [20/100], Validation Loss: 2.2885\n",
      "Epoch [21/100], Train Loss: 1036.5320\n",
      "Epoch [21/100], Validation Loss: 2.3179\n",
      "Epoch [22/100], Train Loss: 1014.3872\n",
      "Epoch [22/100], Validation Loss: 2.3045\n",
      "Epoch [23/100], Train Loss: 1037.1062\n",
      "Epoch [23/100], Validation Loss: 2.2308\n",
      "Epoch [24/100], Train Loss: 1005.4356\n",
      "Epoch [24/100], Validation Loss: 2.1967\n",
      "Epoch [25/100], Train Loss: 1002.5793\n",
      "Epoch [25/100], Validation Loss: 2.2230\n",
      "Epoch [26/100], Train Loss: 1051.0133\n",
      "Epoch [26/100], Validation Loss: 2.3287\n",
      "Epoch [27/100], Train Loss: 1018.3257\n",
      "Epoch [27/100], Validation Loss: 2.2516\n",
      "Epoch [28/100], Train Loss: 1009.7713\n",
      "Epoch [28/100], Validation Loss: 2.2537\n",
      "Epoch [29/100], Train Loss: 991.1795\n",
      "Epoch [29/100], Validation Loss: 2.3128\n",
      "Epoch [30/100], Train Loss: 993.7103\n",
      "Epoch [30/100], Validation Loss: 2.2477\n",
      "Epoch [31/100], Train Loss: 1004.1855\n",
      "Epoch [31/100], Validation Loss: 2.2585\n",
      "Epoch [32/100], Train Loss: 969.5578\n",
      "Epoch [32/100], Validation Loss: 2.3156\n",
      "Epoch [33/100], Train Loss: 979.1799\n",
      "Epoch [33/100], Validation Loss: 2.2443\n",
      "Epoch [34/100], Train Loss: 994.7566\n",
      "Epoch [34/100], Validation Loss: 2.2830\n",
      "Epoch [35/100], Train Loss: 979.9444\n",
      "Epoch [35/100], Validation Loss: 2.2592\n",
      "Epoch [36/100], Train Loss: 978.2935\n",
      "Epoch [36/100], Validation Loss: 2.3116\n",
      "Epoch [37/100], Train Loss: 968.0814\n",
      "Epoch [37/100], Validation Loss: 2.4082\n",
      "Epoch [38/100], Train Loss: 999.1106\n",
      "Epoch [38/100], Validation Loss: 2.4581\n",
      "Epoch [39/100], Train Loss: 957.3441\n",
      "Epoch [39/100], Validation Loss: 2.3220\n",
      "Epoch [40/100], Train Loss: 938.0931\n",
      "Epoch [40/100], Validation Loss: 2.4399\n",
      "Epoch [41/100], Train Loss: 972.3103\n",
      "Epoch [41/100], Validation Loss: 2.3442\n",
      "Epoch [42/100], Train Loss: 963.7207\n",
      "Epoch [42/100], Validation Loss: 2.3693\n",
      "Epoch [43/100], Train Loss: 965.8034\n",
      "Epoch [43/100], Validation Loss: 2.4161\n",
      "Epoch [44/100], Train Loss: 974.2278\n",
      "Epoch [44/100], Validation Loss: 2.3545\n",
      "Epoch [45/100], Train Loss: 965.8654\n",
      "Epoch [45/100], Validation Loss: 2.4294\n",
      "Epoch [46/100], Train Loss: 945.0409\n",
      "Epoch [46/100], Validation Loss: 2.4324\n",
      "Epoch [47/100], Train Loss: 927.7707\n",
      "Epoch [47/100], Validation Loss: 2.3711\n",
      "Epoch [48/100], Train Loss: 971.4403\n",
      "Epoch [48/100], Validation Loss: 2.3229\n",
      "Epoch [49/100], Train Loss: 948.5527\n",
      "Epoch [49/100], Validation Loss: 2.3129\n",
      "Epoch [50/100], Train Loss: 961.3664\n",
      "Epoch [50/100], Validation Loss: 2.4047\n",
      "Epoch [51/100], Train Loss: 927.1003\n",
      "Epoch [51/100], Validation Loss: 2.3559\n",
      "Epoch [52/100], Train Loss: 941.0375\n",
      "Epoch [52/100], Validation Loss: 2.3812\n",
      "Epoch [53/100], Train Loss: 945.4766\n",
      "Epoch [53/100], Validation Loss: 2.4783\n",
      "Epoch [54/100], Train Loss: 941.1322\n",
      "Epoch [54/100], Validation Loss: 2.3583\n",
      "Epoch [55/100], Train Loss: 932.8646\n",
      "Epoch [55/100], Validation Loss: 2.3255\n",
      "Epoch [56/100], Train Loss: 912.8050\n",
      "Epoch [56/100], Validation Loss: 2.4958\n",
      "Epoch [57/100], Train Loss: 921.3139\n",
      "Epoch [57/100], Validation Loss: 2.3669\n",
      "Epoch [58/100], Train Loss: 947.9968\n",
      "Epoch [58/100], Validation Loss: 2.5350\n",
      "Epoch [59/100], Train Loss: 936.5992\n",
      "Epoch [59/100], Validation Loss: 2.3445\n",
      "Epoch [60/100], Train Loss: 929.8250\n",
      "Epoch [60/100], Validation Loss: 2.3546\n",
      "Epoch [61/100], Train Loss: 908.4747\n",
      "Epoch [61/100], Validation Loss: 2.4559\n",
      "Epoch [62/100], Train Loss: 939.9953\n",
      "Epoch [62/100], Validation Loss: 2.3540\n",
      "Epoch [63/100], Train Loss: 950.6595\n",
      "Epoch [63/100], Validation Loss: 2.4736\n",
      "Epoch [64/100], Train Loss: 916.2813\n",
      "Epoch [64/100], Validation Loss: 2.3869\n",
      "Epoch [65/100], Train Loss: 915.2687\n",
      "Epoch [65/100], Validation Loss: 2.4683\n",
      "Epoch [66/100], Train Loss: 923.3926\n",
      "Epoch [66/100], Validation Loss: 2.3871\n",
      "Epoch [67/100], Train Loss: 930.0868\n",
      "Epoch [67/100], Validation Loss: 2.4586\n",
      "Epoch [68/100], Train Loss: 911.2099\n",
      "Epoch [68/100], Validation Loss: 2.4168\n",
      "Epoch [69/100], Train Loss: 892.5658\n",
      "Epoch [69/100], Validation Loss: 2.4197\n",
      "Epoch [70/100], Train Loss: 936.6550\n",
      "Epoch [70/100], Validation Loss: 2.3421\n",
      "Epoch [71/100], Train Loss: 919.1565\n",
      "Epoch [71/100], Validation Loss: 2.5032\n",
      "Epoch [72/100], Train Loss: 895.6424\n",
      "Epoch [72/100], Validation Loss: 2.5171\n",
      "Epoch [73/100], Train Loss: 874.2320\n",
      "Epoch [73/100], Validation Loss: 2.3426\n",
      "Epoch [74/100], Train Loss: 926.6544\n",
      "Epoch [74/100], Validation Loss: 2.5281\n",
      "Epoch [75/100], Train Loss: 933.5096\n",
      "Epoch [75/100], Validation Loss: 2.4936\n",
      "Epoch [76/100], Train Loss: 917.4966\n",
      "Epoch [76/100], Validation Loss: 2.6937\n",
      "Epoch [77/100], Train Loss: 922.7874\n",
      "Epoch [77/100], Validation Loss: 2.5073\n",
      "Epoch [78/100], Train Loss: 929.6117\n",
      "Epoch [78/100], Validation Loss: 2.5041\n",
      "Epoch [79/100], Train Loss: 913.4048\n",
      "Epoch [79/100], Validation Loss: 2.5023\n",
      "Epoch [80/100], Train Loss: 913.0227\n",
      "Epoch [80/100], Validation Loss: 2.4626\n",
      "Epoch [81/100], Train Loss: 898.6921\n",
      "Epoch [81/100], Validation Loss: 2.5164\n",
      "Epoch [82/100], Train Loss: 884.6682\n",
      "Epoch [82/100], Validation Loss: 2.4932\n",
      "Epoch [83/100], Train Loss: 909.8215\n",
      "Epoch [83/100], Validation Loss: 2.4218\n",
      "Epoch [84/100], Train Loss: 881.2043\n",
      "Epoch [84/100], Validation Loss: 2.4418\n",
      "Epoch [85/100], Train Loss: 889.4473\n",
      "Epoch [85/100], Validation Loss: 2.5667\n",
      "Epoch [86/100], Train Loss: 877.7410\n",
      "Epoch [86/100], Validation Loss: 2.5530\n",
      "Epoch [87/100], Train Loss: 883.7088\n",
      "Epoch [87/100], Validation Loss: 2.4176\n",
      "Epoch [88/100], Train Loss: 899.5734\n",
      "Epoch [88/100], Validation Loss: 2.6034\n",
      "Epoch [89/100], Train Loss: 876.8316\n",
      "Epoch [89/100], Validation Loss: 2.3682\n",
      "Epoch [90/100], Train Loss: 888.1155\n",
      "Epoch [90/100], Validation Loss: 2.5066\n",
      "Epoch [91/100], Train Loss: 893.7487\n",
      "Epoch [91/100], Validation Loss: 2.5188\n",
      "Epoch [92/100], Train Loss: 901.1380\n",
      "Epoch [92/100], Validation Loss: 2.5668\n",
      "Epoch [93/100], Train Loss: 872.3384\n",
      "Epoch [93/100], Validation Loss: 2.4604\n",
      "Epoch [94/100], Train Loss: 881.4406\n",
      "Epoch [94/100], Validation Loss: 2.5722\n",
      "Epoch [95/100], Train Loss: 891.2105\n",
      "Epoch [95/100], Validation Loss: 2.5475\n",
      "Epoch [96/100], Train Loss: 878.6368\n",
      "Epoch [96/100], Validation Loss: 2.4713\n",
      "Epoch [97/100], Train Loss: 885.5260\n",
      "Epoch [97/100], Validation Loss: 2.5064\n",
      "Epoch [98/100], Train Loss: 890.1081\n",
      "Epoch [98/100], Validation Loss: 2.4441\n",
      "Epoch [99/100], Train Loss: 886.9021\n",
      "Epoch [99/100], Validation Loss: 2.5071\n",
      "Epoch [100/100], Train Loss: 879.0490\n",
      "Epoch [100/100], Validation Loss: 2.4764\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model = ImprovedNN(input_size, 512,256, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for features, targets in val_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_X = X.iloc[data[data['ID'] == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_y = y.iloc[data[data['ID'] == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name not in ['fc3.weight', 'fc3.bias']:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 183.4544, Validation Loss: 1.2876\n",
      "Epoch [2/100], Train Loss: 73.8927, Validation Loss: 0.8941\n",
      "Epoch [3/100], Train Loss: 61.0796, Validation Loss: 0.8175\n",
      "Epoch [4/100], Train Loss: 54.8768, Validation Loss: 0.7309\n",
      "Epoch [5/100], Train Loss: 49.8744, Validation Loss: 0.6839\n",
      "Epoch [6/100], Train Loss: 47.7828, Validation Loss: 0.6363\n",
      "Epoch [7/100], Train Loss: 44.9456, Validation Loss: 0.5979\n",
      "Epoch [8/100], Train Loss: 44.0380, Validation Loss: 0.5806\n",
      "Epoch [9/100], Train Loss: 43.2861, Validation Loss: 0.5768\n",
      "Epoch [10/100], Train Loss: 42.2473, Validation Loss: 0.5523\n",
      "Epoch [11/100], Train Loss: 41.8552, Validation Loss: 0.5574\n",
      "Epoch [12/100], Train Loss: 41.7423, Validation Loss: 0.5359\n",
      "Epoch [13/100], Train Loss: 41.9519, Validation Loss: 0.5319\n",
      "Epoch [14/100], Train Loss: 40.8145, Validation Loss: 0.5274\n",
      "Epoch [15/100], Train Loss: 39.6120, Validation Loss: 0.5300\n",
      "Epoch [16/100], Train Loss: 40.0484, Validation Loss: 0.5306\n",
      "Epoch [17/100], Train Loss: 40.5011, Validation Loss: 0.5173\n",
      "Epoch [18/100], Train Loss: 41.2299, Validation Loss: 0.5168\n",
      "Epoch [19/100], Train Loss: 40.2203, Validation Loss: 0.5143\n",
      "Epoch [20/100], Train Loss: 39.6578, Validation Loss: 0.5214\n",
      "Epoch [21/100], Train Loss: 39.4935, Validation Loss: 0.5155\n",
      "Epoch [22/100], Train Loss: 39.9706, Validation Loss: 0.5112\n",
      "Epoch [23/100], Train Loss: 40.4921, Validation Loss: 0.5037\n",
      "Epoch [24/100], Train Loss: 39.7763, Validation Loss: 0.5243\n",
      "Epoch [25/100], Train Loss: 38.9823, Validation Loss: 0.5056\n",
      "Epoch [26/100], Train Loss: 38.7236, Validation Loss: 0.5058\n",
      "Epoch [27/100], Train Loss: 38.5884, Validation Loss: 0.5087\n",
      "Epoch [28/100], Train Loss: 39.7710, Validation Loss: 0.5037\n",
      "Epoch [29/100], Train Loss: 39.0314, Validation Loss: 0.5033\n",
      "Epoch [30/100], Train Loss: 38.8587, Validation Loss: 0.5029\n",
      "Epoch [31/100], Train Loss: 38.0582, Validation Loss: 0.5129\n",
      "Epoch [32/100], Train Loss: 38.8878, Validation Loss: 0.5160\n",
      "Epoch [33/100], Train Loss: 39.1086, Validation Loss: 0.5058\n",
      "Epoch [34/100], Train Loss: 38.3004, Validation Loss: 0.4938\n",
      "Epoch [35/100], Train Loss: 40.6242, Validation Loss: 0.5030\n",
      "Epoch [36/100], Train Loss: 38.0621, Validation Loss: 0.4990\n",
      "Epoch [37/100], Train Loss: 38.7968, Validation Loss: 0.4953\n",
      "Epoch [38/100], Train Loss: 37.3358, Validation Loss: 0.5030\n",
      "Epoch [39/100], Train Loss: 38.4079, Validation Loss: 0.5062\n",
      "Epoch [40/100], Train Loss: 39.7374, Validation Loss: 0.5011\n",
      "Epoch [41/100], Train Loss: 38.7385, Validation Loss: 0.4981\n",
      "Epoch [42/100], Train Loss: 38.7447, Validation Loss: 0.4979\n",
      "Epoch [43/100], Train Loss: 40.5254, Validation Loss: 0.4935\n",
      "Epoch [44/100], Train Loss: 39.1995, Validation Loss: 0.5060\n",
      "Epoch [45/100], Train Loss: 37.9423, Validation Loss: 0.4976\n",
      "Epoch [46/100], Train Loss: 38.8088, Validation Loss: 0.5004\n",
      "Epoch [47/100], Train Loss: 39.3823, Validation Loss: 0.4939\n",
      "Epoch [48/100], Train Loss: 38.5017, Validation Loss: 0.4887\n",
      "Epoch [49/100], Train Loss: 39.4186, Validation Loss: 0.5016\n",
      "Epoch [50/100], Train Loss: 38.2262, Validation Loss: 0.4908\n",
      "Epoch [51/100], Train Loss: 38.4176, Validation Loss: 0.5017\n",
      "Epoch [52/100], Train Loss: 37.5741, Validation Loss: 0.4997\n",
      "Epoch [53/100], Train Loss: 38.6223, Validation Loss: 0.5053\n",
      "Epoch [54/100], Train Loss: 39.5392, Validation Loss: 0.4984\n",
      "Epoch [55/100], Train Loss: 39.6673, Validation Loss: 0.4976\n",
      "Epoch [56/100], Train Loss: 39.7699, Validation Loss: 0.4895\n",
      "Epoch [57/100], Train Loss: 38.4092, Validation Loss: 0.4929\n",
      "Epoch [58/100], Train Loss: 37.7242, Validation Loss: 0.5084\n",
      "Epoch [59/100], Train Loss: 39.6979, Validation Loss: 0.5034\n",
      "Epoch [60/100], Train Loss: 39.2058, Validation Loss: 0.5055\n",
      "Epoch [61/100], Train Loss: 38.3495, Validation Loss: 0.4968\n",
      "Epoch [62/100], Train Loss: 38.8641, Validation Loss: 0.4992\n",
      "Epoch [63/100], Train Loss: 39.4763, Validation Loss: 0.4956\n",
      "Epoch [64/100], Train Loss: 39.0098, Validation Loss: 0.4932\n",
      "Epoch [65/100], Train Loss: 39.0214, Validation Loss: 0.5071\n",
      "Epoch [66/100], Train Loss: 39.1973, Validation Loss: 0.4906\n",
      "Epoch [67/100], Train Loss: 40.3698, Validation Loss: 0.5038\n",
      "Epoch [68/100], Train Loss: 38.3191, Validation Loss: 0.4952\n",
      "Epoch [69/100], Train Loss: 40.1607, Validation Loss: 0.4972\n",
      "Epoch [70/100], Train Loss: 38.1726, Validation Loss: 0.4958\n",
      "Epoch [71/100], Train Loss: 38.4036, Validation Loss: 0.5043\n",
      "Epoch [72/100], Train Loss: 38.0472, Validation Loss: 0.5006\n",
      "Epoch [73/100], Train Loss: 38.6029, Validation Loss: 0.4909\n",
      "Epoch [74/100], Train Loss: 39.0276, Validation Loss: 0.5034\n",
      "Epoch [75/100], Train Loss: 37.9338, Validation Loss: 0.4964\n",
      "Epoch [76/100], Train Loss: 39.4757, Validation Loss: 0.5032\n",
      "Epoch [77/100], Train Loss: 37.2717, Validation Loss: 0.4996\n",
      "Epoch [78/100], Train Loss: 40.7499, Validation Loss: 0.4931\n",
      "Epoch [79/100], Train Loss: 39.7569, Validation Loss: 0.5025\n",
      "Epoch [80/100], Train Loss: 38.3069, Validation Loss: 0.4998\n",
      "Epoch [81/100], Train Loss: 38.5389, Validation Loss: 0.5000\n",
      "Epoch [82/100], Train Loss: 37.7870, Validation Loss: 0.5059\n",
      "Epoch [83/100], Train Loss: 38.1663, Validation Loss: 0.5160\n",
      "Epoch [84/100], Train Loss: 38.3339, Validation Loss: 0.4987\n",
      "Epoch [85/100], Train Loss: 38.5492, Validation Loss: 0.5103\n",
      "Epoch [86/100], Train Loss: 38.3740, Validation Loss: 0.4947\n",
      "Epoch [87/100], Train Loss: 38.7603, Validation Loss: 0.5127\n",
      "Epoch [88/100], Train Loss: 39.8114, Validation Loss: 0.4918\n",
      "Epoch [89/100], Train Loss: 38.9240, Validation Loss: 0.5049\n",
      "Epoch [90/100], Train Loss: 38.5023, Validation Loss: 0.4915\n",
      "Epoch [91/100], Train Loss: 38.7502, Validation Loss: 0.5086\n",
      "Epoch [92/100], Train Loss: 38.9443, Validation Loss: 0.4968\n",
      "Epoch [93/100], Train Loss: 39.3473, Validation Loss: 0.5081\n",
      "Epoch [94/100], Train Loss: 39.1674, Validation Loss: 0.4875\n",
      "Epoch [95/100], Train Loss: 39.3415, Validation Loss: 0.5016\n",
      "Epoch [96/100], Train Loss: 39.0055, Validation Loss: 0.4995\n",
      "Epoch [97/100], Train Loss: 38.1793, Validation Loss: 0.5009\n",
      "Epoch [98/100], Train Loss: 38.3525, Validation Loss: 0.5058\n",
      "Epoch [99/100], Train Loss: 39.0934, Validation Loss: 0.4973\n",
      "Epoch [100/100], Train Loss: 40.5514, Validation Loss: 0.5014\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "#Finetune\n",
    "new_epochs=100\n",
    "X_train, X_test, y_train, y_test = train_test_split(finetune_X,finetune_y,test_size=0.33,random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "for epoch in range(new_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for features, targets in val_loader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 111.5858, Validation Loss: 1.1007\n",
      "Epoch [2/200], Train Loss: 107.7362, Validation Loss: 1.1024\n",
      "Epoch [3/200], Train Loss: 109.1536, Validation Loss: 1.1146\n",
      "Epoch [4/200], Train Loss: 109.2053, Validation Loss: 1.1130\n",
      "Epoch [5/200], Train Loss: 110.6679, Validation Loss: 1.1169\n",
      "Epoch [6/200], Train Loss: 109.0765, Validation Loss: 1.1101\n",
      "Epoch [7/200], Train Loss: 109.7284, Validation Loss: 1.1063\n",
      "Epoch [8/200], Train Loss: 106.9332, Validation Loss: 1.1079\n",
      "Epoch [9/200], Train Loss: 110.2005, Validation Loss: 1.1067\n",
      "Epoch [10/200], Train Loss: 106.7441, Validation Loss: 1.1096\n",
      "Epoch [11/200], Train Loss: 110.4850, Validation Loss: 1.1169\n",
      "Epoch [12/200], Train Loss: 110.6533, Validation Loss: 1.1076\n",
      "Epoch [13/200], Train Loss: 108.4519, Validation Loss: 1.1293\n",
      "Epoch [14/200], Train Loss: 108.8656, Validation Loss: 1.1018\n",
      "Epoch [15/200], Train Loss: 109.2217, Validation Loss: 1.1092\n",
      "Epoch [16/200], Train Loss: 110.4191, Validation Loss: 1.1240\n",
      "Epoch [17/200], Train Loss: 109.2020, Validation Loss: 1.1001\n",
      "Epoch [18/200], Train Loss: 111.1245, Validation Loss: 1.0968\n",
      "Epoch [19/200], Train Loss: 111.2659, Validation Loss: 1.0894\n",
      "Epoch [20/200], Train Loss: 110.3148, Validation Loss: 1.1111\n",
      "Epoch [21/200], Train Loss: 113.3768, Validation Loss: 1.0971\n",
      "Epoch [22/200], Train Loss: 110.2195, Validation Loss: 1.1129\n",
      "Epoch [23/200], Train Loss: 109.8883, Validation Loss: 1.0919\n",
      "Epoch [24/200], Train Loss: 112.5439, Validation Loss: 1.1220\n",
      "Epoch [25/200], Train Loss: 110.1316, Validation Loss: 1.1210\n",
      "Epoch [26/200], Train Loss: 107.6155, Validation Loss: 1.1046\n",
      "Epoch [27/200], Train Loss: 108.9100, Validation Loss: 1.1090\n",
      "Epoch [28/200], Train Loss: 108.7357, Validation Loss: 1.1096\n",
      "Epoch [29/200], Train Loss: 108.2768, Validation Loss: 1.0848\n",
      "Epoch [30/200], Train Loss: 110.5586, Validation Loss: 1.0947\n",
      "Epoch [31/200], Train Loss: 108.1036, Validation Loss: 1.0868\n",
      "Epoch [32/200], Train Loss: 112.3369, Validation Loss: 1.1286\n",
      "Epoch [33/200], Train Loss: 110.0873, Validation Loss: 1.1263\n",
      "Epoch [34/200], Train Loss: 108.4372, Validation Loss: 1.0893\n",
      "Epoch [35/200], Train Loss: 108.5573, Validation Loss: 1.1272\n",
      "Epoch [36/200], Train Loss: 112.0500, Validation Loss: 1.1182\n",
      "Epoch [37/200], Train Loss: 108.9579, Validation Loss: 1.1130\n",
      "Epoch [38/200], Train Loss: 109.8597, Validation Loss: 1.1054\n",
      "Epoch [39/200], Train Loss: 109.9479, Validation Loss: 1.0984\n",
      "Epoch [40/200], Train Loss: 109.6598, Validation Loss: 1.1135\n",
      "Epoch [41/200], Train Loss: 110.8735, Validation Loss: 1.0975\n",
      "Epoch [42/200], Train Loss: 110.0034, Validation Loss: 1.1028\n",
      "Epoch [43/200], Train Loss: 109.1211, Validation Loss: 1.1133\n",
      "Epoch [44/200], Train Loss: 110.8545, Validation Loss: 1.0993\n",
      "Epoch [45/200], Train Loss: 108.6119, Validation Loss: 1.1048\n",
      "Epoch [46/200], Train Loss: 110.2923, Validation Loss: 1.0988\n",
      "Epoch [47/200], Train Loss: 109.6483, Validation Loss: 1.1049\n",
      "Epoch [48/200], Train Loss: 109.1733, Validation Loss: 1.1304\n",
      "Epoch [49/200], Train Loss: 110.4985, Validation Loss: 1.1105\n",
      "Epoch [50/200], Train Loss: 112.2620, Validation Loss: 1.0904\n",
      "Epoch [51/200], Train Loss: 110.1536, Validation Loss: 1.1081\n",
      "Epoch [52/200], Train Loss: 110.3748, Validation Loss: 1.1129\n",
      "Epoch [53/200], Train Loss: 111.3073, Validation Loss: 1.1165\n",
      "Epoch [54/200], Train Loss: 110.6926, Validation Loss: 1.0840\n",
      "Epoch [55/200], Train Loss: 111.8200, Validation Loss: 1.1112\n",
      "Epoch [56/200], Train Loss: 107.5349, Validation Loss: 1.0921\n",
      "Epoch [57/200], Train Loss: 108.3423, Validation Loss: 1.0987\n",
      "Epoch [58/200], Train Loss: 108.1547, Validation Loss: 1.0960\n",
      "Epoch [59/200], Train Loss: 108.8930, Validation Loss: 1.1006\n",
      "Epoch [60/200], Train Loss: 107.4843, Validation Loss: 1.0973\n",
      "Epoch [61/200], Train Loss: 111.7751, Validation Loss: 1.1145\n",
      "Epoch [62/200], Train Loss: 107.0863, Validation Loss: 1.1072\n",
      "Epoch [63/200], Train Loss: 109.9416, Validation Loss: 1.1191\n",
      "Epoch [64/200], Train Loss: 108.5560, Validation Loss: 1.1228\n",
      "Epoch [65/200], Train Loss: 113.3743, Validation Loss: 1.1097\n",
      "Epoch [66/200], Train Loss: 108.7513, Validation Loss: 1.1043\n",
      "Epoch [67/200], Train Loss: 107.7612, Validation Loss: 1.0925\n",
      "Epoch [68/200], Train Loss: 111.4924, Validation Loss: 1.1061\n",
      "Epoch [69/200], Train Loss: 113.2600, Validation Loss: 1.1021\n",
      "Epoch [70/200], Train Loss: 112.8996, Validation Loss: 1.1120\n",
      "Epoch [71/200], Train Loss: 112.2444, Validation Loss: 1.0953\n",
      "Epoch [72/200], Train Loss: 111.6618, Validation Loss: 1.1003\n",
      "Epoch [73/200], Train Loss: 110.8674, Validation Loss: 1.1007\n",
      "Epoch [74/200], Train Loss: 108.7289, Validation Loss: 1.1035\n",
      "Epoch [75/200], Train Loss: 107.8619, Validation Loss: 1.1200\n",
      "Epoch [76/200], Train Loss: 110.1515, Validation Loss: 1.1054\n",
      "Epoch [77/200], Train Loss: 106.3748, Validation Loss: 1.1190\n",
      "Epoch [78/200], Train Loss: 108.3334, Validation Loss: 1.0899\n",
      "Epoch [79/200], Train Loss: 108.8564, Validation Loss: 1.1080\n",
      "Epoch [80/200], Train Loss: 109.2336, Validation Loss: 1.0973\n",
      "Epoch [81/200], Train Loss: 110.1977, Validation Loss: 1.1123\n",
      "Epoch [82/200], Train Loss: 110.0014, Validation Loss: 1.0994\n",
      "Epoch [83/200], Train Loss: 109.0470, Validation Loss: 1.1095\n",
      "Epoch [84/200], Train Loss: 110.8213, Validation Loss: 1.1014\n",
      "Epoch [85/200], Train Loss: 108.3712, Validation Loss: 1.0987\n",
      "Epoch [86/200], Train Loss: 109.8462, Validation Loss: 1.1057\n",
      "Epoch [87/200], Train Loss: 109.0123, Validation Loss: 1.1100\n",
      "Epoch [88/200], Train Loss: 111.9014, Validation Loss: 1.0939\n",
      "Epoch [89/200], Train Loss: 106.1439, Validation Loss: 1.1147\n",
      "Epoch [90/200], Train Loss: 111.0126, Validation Loss: 1.0967\n",
      "Epoch [91/200], Train Loss: 109.7181, Validation Loss: 1.1002\n",
      "Epoch [92/200], Train Loss: 111.5629, Validation Loss: 1.1045\n",
      "Epoch [93/200], Train Loss: 107.1674, Validation Loss: 1.1015\n",
      "Epoch [94/200], Train Loss: 109.1439, Validation Loss: 1.1292\n",
      "Epoch [95/200], Train Loss: 108.7267, Validation Loss: 1.0987\n",
      "Epoch [96/200], Train Loss: 111.6156, Validation Loss: 1.1114\n",
      "Epoch [97/200], Train Loss: 108.6730, Validation Loss: 1.0978\n",
      "Epoch [98/200], Train Loss: 109.3319, Validation Loss: 1.1146\n",
      "Epoch [99/200], Train Loss: 108.7155, Validation Loss: 1.1071\n",
      "Epoch [100/200], Train Loss: 109.1073, Validation Loss: 1.1089\n",
      "Epoch [101/200], Train Loss: 109.6391, Validation Loss: 1.0977\n",
      "Epoch [102/200], Train Loss: 110.6643, Validation Loss: 1.1178\n",
      "Epoch [103/200], Train Loss: 108.6309, Validation Loss: 1.1212\n",
      "Epoch [104/200], Train Loss: 106.7555, Validation Loss: 1.1079\n",
      "Epoch [105/200], Train Loss: 110.5193, Validation Loss: 1.1065\n",
      "Epoch [106/200], Train Loss: 109.6021, Validation Loss: 1.0972\n",
      "Epoch [107/200], Train Loss: 108.1275, Validation Loss: 1.1012\n",
      "Epoch [108/200], Train Loss: 112.8325, Validation Loss: 1.1077\n",
      "Epoch [109/200], Train Loss: 108.5305, Validation Loss: 1.1217\n",
      "Epoch [110/200], Train Loss: 111.9854, Validation Loss: 1.1192\n",
      "Epoch [111/200], Train Loss: 110.9011, Validation Loss: 1.0968\n",
      "Epoch [112/200], Train Loss: 108.7311, Validation Loss: 1.1037\n",
      "Epoch [113/200], Train Loss: 110.1656, Validation Loss: 1.0997\n",
      "Epoch [114/200], Train Loss: 109.4285, Validation Loss: 1.0955\n",
      "Epoch [115/200], Train Loss: 109.5795, Validation Loss: 1.1023\n",
      "Epoch [116/200], Train Loss: 112.8959, Validation Loss: 1.1111\n",
      "Epoch [117/200], Train Loss: 111.0341, Validation Loss: 1.0805\n",
      "Epoch [118/200], Train Loss: 111.1856, Validation Loss: 1.1075\n",
      "Epoch [119/200], Train Loss: 108.4274, Validation Loss: 1.1077\n",
      "Epoch [120/200], Train Loss: 111.5112, Validation Loss: 1.1069\n",
      "Epoch [121/200], Train Loss: 112.3010, Validation Loss: 1.0977\n",
      "Epoch [122/200], Train Loss: 111.5119, Validation Loss: 1.1181\n",
      "Epoch [123/200], Train Loss: 111.7952, Validation Loss: 1.1115\n",
      "Epoch [124/200], Train Loss: 110.8714, Validation Loss: 1.1144\n",
      "Epoch [125/200], Train Loss: 108.7130, Validation Loss: 1.1129\n",
      "Epoch [126/200], Train Loss: 107.8797, Validation Loss: 1.1335\n",
      "Epoch [127/200], Train Loss: 109.6633, Validation Loss: 1.1003\n",
      "Epoch [128/200], Train Loss: 110.3560, Validation Loss: 1.1233\n",
      "Epoch [129/200], Train Loss: 109.6689, Validation Loss: 1.1057\n",
      "Epoch [130/200], Train Loss: 108.3568, Validation Loss: 1.1032\n",
      "Epoch [131/200], Train Loss: 106.1816, Validation Loss: 1.0934\n",
      "Epoch [132/200], Train Loss: 111.4383, Validation Loss: 1.1174\n",
      "Epoch [133/200], Train Loss: 111.7228, Validation Loss: 1.1108\n",
      "Epoch [134/200], Train Loss: 109.6861, Validation Loss: 1.1029\n",
      "Epoch [135/200], Train Loss: 112.2317, Validation Loss: 1.1375\n",
      "Epoch [136/200], Train Loss: 110.9990, Validation Loss: 1.1082\n",
      "Epoch [137/200], Train Loss: 108.9435, Validation Loss: 1.1201\n",
      "Epoch [138/200], Train Loss: 109.7889, Validation Loss: 1.1182\n",
      "Epoch [139/200], Train Loss: 110.1830, Validation Loss: 1.1058\n",
      "Epoch [140/200], Train Loss: 111.2556, Validation Loss: 1.1082\n",
      "Epoch [141/200], Train Loss: 111.5573, Validation Loss: 1.1165\n",
      "Epoch [142/200], Train Loss: 109.4112, Validation Loss: 1.0837\n",
      "Epoch [143/200], Train Loss: 112.8708, Validation Loss: 1.1037\n",
      "Epoch [144/200], Train Loss: 109.5668, Validation Loss: 1.1114\n",
      "Epoch [145/200], Train Loss: 105.2225, Validation Loss: 1.1104\n",
      "Epoch [146/200], Train Loss: 108.9677, Validation Loss: 1.1081\n",
      "Epoch [147/200], Train Loss: 110.2867, Validation Loss: 1.1184\n",
      "Epoch [148/200], Train Loss: 107.4403, Validation Loss: 1.0871\n",
      "Epoch [149/200], Train Loss: 112.4920, Validation Loss: 1.1176\n",
      "Epoch [150/200], Train Loss: 108.6899, Validation Loss: 1.1181\n",
      "Epoch [151/200], Train Loss: 110.1706, Validation Loss: 1.1112\n",
      "Epoch [152/200], Train Loss: 108.1913, Validation Loss: 1.1124\n",
      "Epoch [153/200], Train Loss: 109.2183, Validation Loss: 1.0999\n",
      "Epoch [154/200], Train Loss: 111.3958, Validation Loss: 1.1097\n",
      "Epoch [155/200], Train Loss: 107.9015, Validation Loss: 1.1030\n",
      "Epoch [156/200], Train Loss: 110.4549, Validation Loss: 1.1292\n",
      "Epoch [157/200], Train Loss: 110.1106, Validation Loss: 1.1144\n",
      "Epoch [158/200], Train Loss: 109.3797, Validation Loss: 1.1098\n",
      "Epoch [159/200], Train Loss: 109.9704, Validation Loss: 1.1285\n",
      "Epoch [160/200], Train Loss: 108.4105, Validation Loss: 1.0860\n",
      "Epoch [161/200], Train Loss: 110.6666, Validation Loss: 1.1202\n",
      "Epoch [162/200], Train Loss: 111.1790, Validation Loss: 1.1110\n",
      "Epoch [163/200], Train Loss: 110.4226, Validation Loss: 1.1168\n",
      "Epoch [164/200], Train Loss: 110.2173, Validation Loss: 1.1200\n",
      "Epoch [165/200], Train Loss: 108.8192, Validation Loss: 1.0999\n",
      "Epoch [166/200], Train Loss: 109.2776, Validation Loss: 1.1049\n",
      "Epoch [167/200], Train Loss: 108.6484, Validation Loss: 1.1046\n",
      "Epoch [168/200], Train Loss: 110.5806, Validation Loss: 1.1020\n",
      "Epoch [169/200], Train Loss: 112.2392, Validation Loss: 1.1075\n",
      "Epoch [170/200], Train Loss: 109.4315, Validation Loss: 1.1000\n",
      "Epoch [171/200], Train Loss: 108.8024, Validation Loss: 1.1048\n",
      "Epoch [172/200], Train Loss: 110.4620, Validation Loss: 1.1203\n",
      "Epoch [173/200], Train Loss: 110.8760, Validation Loss: 1.0991\n",
      "Epoch [174/200], Train Loss: 109.6771, Validation Loss: 1.1043\n",
      "Epoch [175/200], Train Loss: 108.9314, Validation Loss: 1.1038\n",
      "Epoch [176/200], Train Loss: 109.2277, Validation Loss: 1.1230\n",
      "Epoch [177/200], Train Loss: 112.4286, Validation Loss: 1.1050\n",
      "Epoch [178/200], Train Loss: 108.9929, Validation Loss: 1.1031\n",
      "Epoch [179/200], Train Loss: 110.6384, Validation Loss: 1.1063\n",
      "Epoch [180/200], Train Loss: 109.6186, Validation Loss: 1.1034\n",
      "Epoch [181/200], Train Loss: 112.3942, Validation Loss: 1.0758\n",
      "Epoch [182/200], Train Loss: 110.3010, Validation Loss: 1.1225\n",
      "Epoch [183/200], Train Loss: 106.9864, Validation Loss: 1.1189\n",
      "Epoch [184/200], Train Loss: 107.4824, Validation Loss: 1.1125\n",
      "Epoch [185/200], Train Loss: 107.1951, Validation Loss: 1.0932\n",
      "Epoch [186/200], Train Loss: 108.8696, Validation Loss: 1.1226\n",
      "Epoch [187/200], Train Loss: 110.3662, Validation Loss: 1.1002\n",
      "Epoch [188/200], Train Loss: 108.6317, Validation Loss: 1.0975\n",
      "Epoch [189/200], Train Loss: 109.8322, Validation Loss: 1.0973\n",
      "Epoch [190/200], Train Loss: 109.0249, Validation Loss: 1.1062\n",
      "Epoch [191/200], Train Loss: 111.6608, Validation Loss: 1.0984\n",
      "Epoch [192/200], Train Loss: 109.6272, Validation Loss: 1.1066\n",
      "Epoch [193/200], Train Loss: 108.5166, Validation Loss: 1.1018\n",
      "Epoch [194/200], Train Loss: 108.6021, Validation Loss: 1.1136\n",
      "Epoch [195/200], Train Loss: 107.7388, Validation Loss: 1.1048\n",
      "Epoch [196/200], Train Loss: 108.9586, Validation Loss: 1.1016\n",
      "Epoch [197/200], Train Loss: 109.7701, Validation Loss: 1.1076\n",
      "Epoch [198/200], Train Loss: 108.0976, Validation Loss: 1.1037\n",
      "Epoch [199/200], Train Loss: 105.9982, Validation Loss: 1.1100\n",
      "Epoch [200/200], Train Loss: 109.3499, Validation Loss: 1.0981\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "#Train from scratch\n",
    "new_epochs=200\n",
    "X_train, X_test, y_train, y_test = train_test_split(finetune_X,finetune_y,test_size=0.33,random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model2 = ImprovedNN(input_size, 512,256, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(new_epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        outputs = model2(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "        \n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for features, targets in val_loader:\n",
    "            outputs = model2(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{new_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
